{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIVRIA : Filtrage colllaboratif - Partie II\n",
    "\n",
    "   Ce jupyter notebook contient le code permettant la prédiction des livres susceptibles de plaire à l'utilisateur. Nous entraînons et mesurons les prédictions ici, mais vous retrouverez la mise en forme des résultats de la prédiction dans le notebook Livra_recommender_system.ipynb.\n",
    "\n",
    "   Nous avons dans un premier temps utilisé notre base de données issue du questionnaire afin de mettre en place des modèles de prédiction de thèmes, modèles basés sur des techniques de filtrage collaboratif. On a pu mesurer la perfomance des modèles et les comparer entre eux en se basant sur un type de mesure d'erreur entre les valeurs des données du set de test et les prédicions des différents modèles.\n",
    "   Maintenant, nous allons pouvoir utiliser la base de données Goodbooks-10k pour réaliser le même travail de filtrage collaboratif mais cet fois-ci la prédiction sera portée sur des livres. \n",
    "   \n",
    "* Le set de données de Goodbooks-10 :\n",
    "http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/\n",
    "\n",
    "Je précise que dans la dernière partie du notebook dataVizualisation&Cleaning_GoodBooks10k.ipynb, avons nettoyé le set de donnée \"**ratings.csv**\" que nous avons enregistré sous le nom \"**df_notes.csv**\" (cf. dossier ./data). Ainsi, nous avons gardé les livres les plus notés et les utilisateurs les plus actifs - ceux qui ont attribués le plus de notes - afin de faciliter le filtrage collaboratif et de contourner le problème de la taille du set complet qui était trop gros pour être utilisé entièrement ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des librairies\n",
    "\n",
    "On commence par importer les librairies utilisées dans ce notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des données\n",
    "\n",
    "Vous pouvez retrouver l'ensemble des données utilisées dans le dossier './data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier 'df_notes.csv'\n",
    "df_notes = pd.read_csv('data/df_notes.csv', sep='\\t')\n",
    "# On supprime la colonne inutile :\n",
    "del df_notes['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_livre</th>\n",
       "      <th>book_id</th>\n",
       "      <th>index_user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5333</td>\n",
       "      <td>509</td>\n",
       "      <td>27448</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>12</td>\n",
       "      <td>6630</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>163</td>\n",
       "      <td>17434</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>168</td>\n",
       "      <td>23576</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>178</td>\n",
       "      <td>13776</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>260</td>\n",
       "      <td>41577</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>293</td>\n",
       "      <td>45493</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>295</td>\n",
       "      <td>3918</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>443</td>\n",
       "      <td>18045</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>3206</td>\n",
       "      <td>451</td>\n",
       "      <td>14177</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_livre  book_id  index_user  user_id  rating\n",
       "0            0     5333         509    27448       5\n",
       "1            1     3206          12     6630       4\n",
       "2            1     3206         163    17434       5\n",
       "3            1     3206         168    23576       5\n",
       "4            1     3206         178    13776       3\n",
       "5            1     3206         260    41577       5\n",
       "6            1     3206         293    45493       3\n",
       "7            1     3206         295     3918       4\n",
       "8            1     3206         443    18045       2\n",
       "9            1     3206         451    14177       5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### On montre un extrait des notes attribuées\n",
    "df_notes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les dimensions de df_notes sont de : (23437, 5)\n",
      "Nombre d'utilisateurs : 608\n",
      "Nombre de livres : 1000\n"
     ]
    }
   ],
   "source": [
    "n_users = df_notes['user_id'].nunique()\n",
    "n_livres = df_notes['book_id'].nunique()\n",
    "print('Les dimensions de df_notes sont de : ' + str(df_notes.shape))\n",
    "print(\"Nombre d'utilisateurs : \" + str(n_users))\n",
    "print(\"Nombre de livres : \" + str(n_livres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des sets d'entraînement et de test\n",
    "\n",
    "Tout comme nous l'avons fait pour les thèmes préalablement (Cf. Partie I du filtrage collaboratif), on sépare le dataset en deux sets distincts : un pour l'entraînement de notre modèle de prédiction et un pour tester ce modèle. On garde 25% des données pour le set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_livre</th>\n",
       "      <th>book_id</th>\n",
       "      <th>index_user</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>146</td>\n",
       "      <td>125</td>\n",
       "      <td>209</td>\n",
       "      <td>22164</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>95</td>\n",
       "      <td>2607</td>\n",
       "      <td>461</td>\n",
       "      <td>15194</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>912</td>\n",
       "      <td>1576</td>\n",
       "      <td>155</td>\n",
       "      <td>25182</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>146</td>\n",
       "      <td>125</td>\n",
       "      <td>104</td>\n",
       "      <td>30283</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19480</th>\n",
       "      <td>830</td>\n",
       "      <td>1018</td>\n",
       "      <td>216</td>\n",
       "      <td>17804</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index_livre  book_id  index_user  user_id  rating\n",
       "3603           146      125         209    22164       4\n",
       "2036            95     2607         461    15194       3\n",
       "21413          912     1576         155    25182       5\n",
       "3567           146      125         104    30283       5\n",
       "19480          830     1018         216    17804       3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df_notes, test_size=0.25)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrage collaboratif\n",
    "\n",
    "On se base toujours sur les deux mêmes modèles pour le filtrage collaboratif : le \"**memory-based**\" et le \"**model-based**\".\n",
    "\n",
    "Pour commencer, on crée une matrice utilisateur-livre pour l'entraînement du modèle de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_matrix = np.zeros((n_users,n_livres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in train_data.itertuples():\n",
    "    train_data_matrix[line[3], line[1]] = line[5]\n",
    "train_data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée ensuite une matrice utilisateur-thème pour tester le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_matrix = np.zeros((n_users,n_livres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in test_data.itertuples():\n",
    "    test_data_matrix[line[3], line[1]] = line[5]\n",
    "test_data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrage collaboratif avec la méthode Memory-Based \n",
    "\n",
    "L'idée sous-jacente derrière le modèle dit \"**memory-based**\" est de calculer et d'utiliser les **similarités** entre utilisateurs et/ou items -ici les thèmes- et d'utiliser ces facteurs comme des \"poids\"  permettant la prédiction d'un thème, d'une note attribuée à un livre, ou autre. \n",
    "\n",
    "Nous allons tester les deux types de filtrage collaboratif:\n",
    "\n",
    "* Item-Item \n",
    "* Utilisateur-Item \n",
    "\n",
    "Nous utilisons le coefficient de similarité. Pour cela, nous importons la fonction \"pairwise_distances\" de Scikit-Learn. \n",
    "\n",
    "On calcule d'abord la similarité entre les utilisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "user_similarity = pairwise.cosine_similarity(train_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.27786086, 0.29377887, ..., 0.07473288, 0.13880146,\n",
       "        0.11708833],\n",
       "       [0.27786086, 1.        , 0.18293123, ..., 0.14522597, 0.10725293,\n",
       "        0.13590262],\n",
       "       [0.29377887, 0.18293123, 1.        , ..., 0.20008829, 0.26288237,\n",
       "        0.16432203],\n",
       "       ...,\n",
       "       [0.07473288, 0.14522597, 0.20008829, ..., 1.        , 0.18764551,\n",
       "        0.15313109],\n",
       "       [0.13880146, 0.10725293, 0.26288237, ..., 0.18764551, 1.        ,\n",
       "        0.19752469],\n",
       "       [0.11708833, 0.13590262, 0.16432203, ..., 0.15313109, 0.19752469,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la similarité entre les livres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity = pairwise.cosine_similarity(train_data_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit une méthode pour réaliser les prédictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(choices, similarity, kind='user'):\n",
    "    \n",
    "    sum_sim = np.array([np.abs(similarity).sum(axis=1)])\n",
    "    sum_sim[sum_sim == 0] = 1    \n",
    "    if kind == 'user':\n",
    "        return similarity.dot(choices) / sum_sim.T\n",
    "    elif kind == 'item':\n",
    "        return choices.dot(similarity) / sum_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode permet de prédire les livres susceptibles d'intéresser un utilisateur. Soit elle prend en considération les livres qui lui plaisent déjà, soit elle regarde les thèmes de prédilection d'autres utilisateurs ayant donné des réponses similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prediction = predict(train_data_matrix, item_similarity, 'item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09012529, 0.22497681, 0.2589533 ],\n",
       "       [0.20688677, 0.20361765, 0.62090655],\n",
       "       [0.12507581, 0.27639341, 0.14598705],\n",
       "       [0.04970811, 0.21190072, 0.22208339],\n",
       "       [0.21616734, 0.24452245, 0.10361473]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_prediction[0:5,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0025917 , 0.0528627 , 0.04487075],\n",
       "       [0.00677169, 0.03871761, 0.0808874 ],\n",
       "       [0.00404404, 0.05115041, 0.02118263],\n",
       "       [0.00171438, 0.04864771, 0.03221118],\n",
       "       [0.0089254 , 0.05878382, 0.01725629]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prediction = predict(train_data_matrix, user_similarity, 'user')\n",
    "user_prediction[0:5,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On mesure la performance du modèle avec le calcul de la RMSE (root-mean-square error), c'est-à-dire la racine carrée de l'erreur quadratique. Cette méthode compare les vraies réponses aux réponses prédites par notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, true_value):\n",
    "    prediction = prediction.flatten()\n",
    "    true_value = true_value.flatten()\n",
    "    return sqrt(mean_squared_error(prediction, true_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE pour la prédiction basée sur la comparaison entre les utilisateurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE basée sur les utilisateurs :  0.42418025225419365\n"
     ]
    }
   ],
   "source": [
    "user_CF_RMSE = rmse(user_prediction, test_data_matrix)\n",
    "print('RMSE basée sur les utilisateurs : ', user_CF_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE basée sur les thèmes :  0.45386822381131486\n"
     ]
    }
   ],
   "source": [
    "item_CF_RMSE = rmse(item_prediction, test_data_matrix)\n",
    "print('RMSE basée sur les thèmes : ', item_CF_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrage collaboratif avec la méthode Model-based\n",
    "\n",
    "La même logique développée dans la partie précédente (cf. I.1 memory-based collaborative filtering) peut être utilisée dans la méthode dite \"model-based\" : les similarités entre utilisateurs et/ou items peuvent être calculées et associées à un *modèle*, et on peut ensuite utiliser ce modèle pour faire nos prédictions. \n",
    "\n",
    "Le filtrage collaboratif dit \"model-based\" repose sur la factorisation de matrice. \n",
    "\n",
    "Nous allons utiliser un algorithme \"SVD-based\" permettant de réduire les dimensions de notre set de données et de guarder les caractéristiques principales, c'est-à-dire déterminantes de nos prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On regarde la proportion d'absence de données dans notre matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de 0 dans la matrice :96.1%\n"
     ]
    }
   ],
   "source": [
    "sparsity=round(1.0-len(df_notes)/float(n_users*n_livres),3)\n",
    "print('Taux de 0 dans la matrice :' +  str(sparsity*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Decompose the train_data_theme_matrix using the SVD method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(train_data_matrix, k = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Crée une matrice diagonale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_diag_matrix=np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Compute the rating predictions from the decomposition values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the model RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9]\n",
      "SVD-based CF RMSE (k=1): 0.42129094728953215\n",
      "SVD-based CF RMSE (k=2): 0.4294439057195919\n",
      "SVD-based CF RMSE (k=3): 0.43681393777181426\n",
      "SVD-based CF RMSE (k=4): 0.44363899960635694\n",
      "SVD-based CF RMSE (k=5): 0.4507822883434278\n",
      "SVD-based CF RMSE (k=6): 0.45809242382741466\n",
      "SVD-based CF RMSE (k=7): 0.4635105890397339\n",
      "SVD-based CF RMSE (k=8): 0.4690355653019209\n",
      "SVD-based CF RMSE (k=9): 0.4749943069111912\n"
     ]
    }
   ],
   "source": [
    "k = np.arange(1,10) \n",
    "print(k)\n",
    "for k in k:\n",
    "    u, s, vt = svds(train_data_matrix, k = k)\n",
    "    s_diag_matrix=np.diag(s)\n",
    "    X_pred = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "    print('SVD-based CF RMSE (k={}): {}'.format(k, str(rmse(X_pred, test_data_matrix))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA avec Scikit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on importe la librairie permettant la réduction de dimension de notre set de données sur les thèmes\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on détermine précisemment le nombre minimum de dimensions à garder pour préserver au moins 95% de la variance caractérisant notre set de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=100 must be between 1 and min(n_samples, n_features)=5 with svd_solver='arpack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-fef669eab8f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# On détermine d, le nombre de dimensions après PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd_solver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcumsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             raise ValueError(\"Unrecognized svd_solver='{0}'\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    489\u001b[0m                              \u001b[0;34m\"svd_solver='%s'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                              % (n_components, min(n_samples, n_features),\n\u001b[0;32m--> 491\u001b[0;31m                                 svd_solver))\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             raise ValueError(\"n_components=%r must be of type int \"\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=100 must be between 1 and min(n_samples, n_features)=5 with svd_solver='arpack'"
     ]
    }
   ],
   "source": [
    "# On détermine d, le nombre de dimensions après PCA\n",
    "pca = PCA(n_components=100, svd_solver='arpack')\n",
    "pca.fit(train_data)\n",
    "print(pca.explained_variance_ratio_)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95)+1\n",
    "print(cumsum)\n",
    "\n",
    "# On trace la variance cumulative en fonction du nombre de dimensions\n",
    "dim=np.arange(1,6)\n",
    "plt.plot(dim, cumsum)\n",
    "axes = plt.gca()\n",
    "plt.axhline(y=0.95,color=\"red\")\n",
    "#plt.axvline(x=13, color=\"black\", linestyle='--')\n",
    "#axes.xaxis.set_ticks(range(1001))\n",
    "#plt.axvspan(12, 13, facecolor='#2ca02c', alpha=0.3)\n",
    "plt.title(\"Variance expliquée en fonction du nombre de dimensions\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Variance cumulative expliquée (%)\")\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()\n",
    "\n",
    "print ('\\n nombre de dimensions du set après PCA : ' + str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela veut dire que l'on ne peut enlever qu'une seule dimension à notre set de données si on souhaite garder assez d'informations pour la prédiction. Cependant, comme nous pouvons le voir sur le graphique, réduire le set à 12 dimensions ne nous ferait pas dépasser de beaucoup la limite de variance cumulative généralement fixée à 95% de celle du set initial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
